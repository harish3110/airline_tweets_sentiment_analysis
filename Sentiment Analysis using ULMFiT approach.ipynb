{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using ULMFiT approach\n",
    "> Implementing and decoding the revolutionary [ULMFiT](https://arxiv.org/abs/1801.06146) approach to fine-tune a language model on a sentiment analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing and importing the necessary libraries \n",
    "# !pip install fastai2 --quiet\n",
    "# !pip install kaggle --quiet\n",
    "\n",
    "from fastai2.text.all import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dataset Download and  basic EDA\n",
    "> Using Kaggle API to download the competition dataset and view the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll be using the [Twitter Airlines Tweets](https://www.kaggle.com/crowdflower/twitter-airline-sentiment#Tweets.csv) dataset from Kaggle to build the Sentiment classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref                                                         title                                             size  lastUpdated          downloadCount  voteCount  usabilityRating  \r\n",
      "----------------------------------------------------------  -----------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \r\n",
      "giovamata/airlinedelaycauses                                Airlines Delay                                    67MB  2019-11-14 06:17:27          13461        176  0.64705884       \r\n",
      "crowdflower/twitter-airline-sentiment                       Twitter US Airline Sentiment                       3MB  2019-10-16 00:04:05          41619        599  0.8235294        \r\n",
      "semihyilmaz/turkish-airlines-daily-stock-prices-since-2013  Turkish Airlines daily stock prices since 2013    22KB  2019-06-20 14:28:03            349         14  1.0              \r\n",
      "ternaryrealm/airlines-passenger-data                        Airlines Passenger Data                           907B  2018-01-31 10:41:40            725          6  0.5882353        \r\n",
      "prajitdatta/data-stories-of-us-airlines                     Data Stories of US Airlines, 1987-2008             1MB  2017-03-11 19:12:00            890         24  0.8235294        \r\n",
      "tanmoyie/airlines-network-optimization                      airlines_network_optimization                      3KB  2018-04-20 20:06:38            228          3  0.29411766       \r\n",
      "chrisbellec/airlines-tweets-sentiments                      Airlines Tweets Sentiments                        56KB  2017-09-07 17:42:07            232          4  0.7647059        \r\n",
      "sjleshrac/airlines-customer-satisfaction                    Airlines Customer satisfaction                     2MB  2020-03-19 15:33:25             92          2  0.5294118        \r\n",
      "karthickveerakumar/airlines                                 Airlines                                          41KB  2017-07-19 16:23:46            377          1  0.4117647        \r\n",
      "yashgoyal401/airlines-data-set                              Airlines Data set                                 54KB  2019-08-30 08:11:26             20          6  0.11764706       \r\n",
      "ahmednour/mmmmmmm                                           Airlines Customer satisfaction                     2MB  2020-04-28 20:21:08             22          1  0.47058824       \r\n",
      "tanmoyie/airlines-network-optimization-cleaned              airlines_network_optimization_cleaned             16KB  2019-04-25 05:32:11            137          2  0.05882353       \r\n",
      "open-flights/airline-database                               Airline Database                                 120KB  2017-08-29 16:52:34           3248         66  0.8235294        \r\n",
      "traceyvanp/airlinefleet                                     Airline Fleets                                    21KB  2017-02-09 17:09:17           2046         37  0.7058824        \r\n",
      "daliabarrios04/airlines                                     Airlines                                          19KB  2018-10-03 13:38:59              4          0  0.125            \r\n",
      "johnsmith46/airlines                                        airlines                                         118KB  2019-02-08 07:55:58              7          0  0.125            \r\n",
      "welkin10/airline-sentiment                                  Airline sentiment                                  1MB  2018-05-27 07:23:18            777         14  0.7058824        \r\n",
      "faa/wildlife-strikes                                        Aircraft Wildlife Strikes, 1990-2015               5MB  2017-02-08 14:51:11           2279         73  0.8235294        \r\n",
      "zernach/2018-airplane-flights                               2018 Airplane Flights                             87MB  2019-11-16 15:13:23            449         29  0.9705882        \r\n",
      "danoozy44/airline-safety                                    Airline Safety                                     1KB  2020-05-28 10:51:50             23          9  1.0              \r\n"
     ]
    }
   ],
   "source": [
    "# Using the kaggle api to search the name of the dataset to download\n",
    "!kaggle datasets list -s 'twitter airlines'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The dataset we would like to download is the 2nd one titled `crowdflower/twitter-airline-sentiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path = '/home/harish3110/Desktop/airline_tweets_sentiment_analysis/'\n",
    "dataset = 'crowdflower/twitter-airline-sentiment'\n",
    "dataset_name = 'twitter-airline-sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/harish3110/Desktop/airline_tweets_sentiment_analysis/data\n",
      "Downloading twitter-airline-sentiment.zip to /home/harish3110/Desktop/airline_tweets_sentiment_analysis/data\n",
      " 78%|█████████████████████████████▊        | 2.00M/2.55M [00:00<00:00, 2.91MB/s]\n",
      "100%|██████████████████████████████████████| 2.55M/2.55M [00:00<00:00, 4.25MB/s]\n",
      "Archive:  twitter-airline-sentiment.zip\n",
      "  inflating: Tweets.csv              \n",
      "  inflating: database.sqlite         \n",
      "/home/harish3110/Desktop/airline_tweets_sentiment_analysis\n"
     ]
    }
   ],
   "source": [
    "# Creating a folder for the dataset\n",
    "!mkdir {path + 'data'}\n",
    "%cd {path + 'data'}\n",
    "\n",
    "# Using the Kaggle API to download dataset\n",
    "!kaggle datasets download {dataset}\n",
    "\n",
    "# Unzip the dataset and delete the respective zip file\n",
    "!unzip {dataset_name + '.zip'}\n",
    "!rm {dataset_name + '.zip'} \n",
    "\n",
    "%cd {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('data/database.sqlite'),Path('data/Tweets.csv')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path(path)\n",
    "Path.BASE_PATH = path\n",
    "(path/'data').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                                                                                             text  \\\n",
       "0                                                                                             @VirginAmerica What @dhepburn said.   \n",
       "1                                                        @VirginAmerica plus you've added commercials to the experience... tacky.   \n",
       "2                                                         @VirginAmerica I didn't today... Must mean I need to take another trip!   \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse   \n",
       "4                                                                         @VirginAmerica and it's a really big bad thing about it   \n",
       "\n",
       "  tweet_coord              tweet_created tweet_location  \\\n",
       "0         NaN  2015-02-24 11:35:52 -0800            NaN   \n",
       "1         NaN  2015-02-24 11:15:59 -0800            NaN   \n",
       "2         NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
       "3         NaN  2015-02-24 11:15:36 -0800            NaN   \n",
       "4         NaN  2015-02-24 11:14:45 -0800            NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0  Eastern Time (US & Canada)  \n",
       "1  Pacific Time (US & Canada)  \n",
       "2  Central Time (US & Canada)  \n",
       "3  Pacific Time (US & Canada)  \n",
       "4  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path/'data/Tweets.csv')\n",
    "\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing distribution of sentiments in dataset\n",
    "df['airline_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset corpus set has 14640 tweets.\n"
     ]
    }
   ],
   "source": [
    "print(f'The dataset corpus set has {len(df)} tweets.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ULMFiT approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Universal Language Model Fine-tuning (ULMFiT) is an inductive transfer learning approach developed by Jeremy Howard and Sebastian Ruder to all the tasks in the domain of natural language processing which sparked the usage of transfer learning in NLP tasks. \n",
    "\n",
    "**The ULMFiT approach to training NLP models is heralded as the ImageNet moment in the domain of Natural Language Processing** \n",
    "\n",
    "The model architecture used in the entire process of the ULMFiT approach is ubiquitous and is the well-known **AWD-LSTM** architecture. \n",
    "\n",
    "The ULMFiT approach can be braodly explained in the 3 major steps as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/2000/1*9n9yv4EalUn76yP1Yffhfw.png 'The ULMFiT Process')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Training a general corpus language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A language model is first trained on a corpus of Wikipedia articles known as Wikitext-103 using a **self-supervised approach**, i.e. using the training labels in itself to train models, in this case training a LM to learn to predict the next word in a sequence. This resulting LM learns the semantics of the english language and captures general features in the different layers. \n",
    "\n",
    "This pretrained language model is trained on 28,595 Wikipedia articles and training process is very expensive and time consuming and is luckily open-sourced in the Fastai library for us to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side Note: Text Pre-processing\n",
    ">Transforming and normalizing texts such that it can be trained on a neural network for language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to image models where all images used for training the image classifier model are resized to the same dimensions in order to be able to collate them correctly in the GPU, text models also needds to be pre-processed to ensure that we can train a NN model on them. \n",
    "\n",
    "Whether it's the articles in the Wikipedia 103 dataset or tweets in airline dataset are of different lengths and can be very long. Thus the tweets corpus i.e. the dataset needs to pre-processed correctly in order to train a neural network on text data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways the pre-processing for textual data can be done and Fastai approach is to apply the following 2 main transforms to texts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "***Note:*** A transform in Fastai is basically an **almost** reversible function that transforms data into another form(encoding) and also has the capability of getting back the original data(decoding) if needed. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Tokenization\n",
    "\n",
    "The first step is to gather all the unique `tokens` in the corpus being used. \n",
    "\n",
    "A `token` can be defined in numerous ways depedning on the person creating the language model based on the granularity level i.e. the smallest part of the text they would like to consider. In the simplest scenario, a word can be considered as the token.\n",
    "\n",
    "So the idea is to get a list of all the unique words used in the general domain corpus(Wikipedia 103 dataset) and our added downstream dataset(airline tweets dataset) to build a vocabulary for training our language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@VirginAmerica What @dhepburn said.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take an example text from our training set to show a tokenization example\n",
    "\n",
    "txt = df['text'].iloc[0]\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#7) ['xxbos','@virginamerica','xxmaj','what','@dhepburn','said','.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the default tokenizer used in Fastai which is that of Spacy called `WordTokenizer`\n",
    "spacy = WordTokenizer() \n",
    "\n",
    "# Wrapping the Spacy tokenizer with a custom Fastai function to make some custom changes to the tokenizer\n",
    "tkn = Tokenizer(spacy) \n",
    "\n",
    "tkn(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = L([i for i in df['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#7) ['xxbos','@virginamerica','xxmaj','what','@dhepburn','said','.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up a tokenizer on the entire dataframe 'train'\n",
    "tok = Tokenizer.from_df(df)\n",
    "tok.setup(df)\n",
    "\n",
    "toks = txts.map(tok)\n",
    "toks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Note:** The special tokens you can see above starting with 'xx' are special fastai tokens added on top of the spacy tokenizer used to indicate certain extra meanings in the text data as follows:\n",
    "\n",
    "- `xxbos`:: Indicates the beginning of a text (here, a review)\n",
    "- `xxmaj`:: Indicates the next word begins with a capital (since we lowercased everything)\n",
    "- `xxunk`:: Indicates the next word is unknown\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above `Tokenizer` is a Fastai transform, which is basically a function with and `encodes` and `decodes` method available to tokenize a text and return it back to **almost** the same initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos @virginamerica xxmaj what @dhepburn said .'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode(toks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we don't get the original string back when applying `decode` is because the default tokenizer used in this case isn't `reversible`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Numericalization\n",
    "\n",
    "The next step in the pre-processing step is to index the tokens created earlier so that they can easily accessed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2, 105,   8,  72,   0, 251,   9])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks)\n",
    "nums = toks.map(num)\n",
    "nums[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorText([  2, 105,   8,  72,   0, 251,   9])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num.encodes(toks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#7) ['xxbos','@virginamerica','xxmaj','what','xxunk','said','.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num.decode(nums[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Fine-tuning pretrained LM to downstream dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite having a vast language model pre-trained, it's always likely that the specific downstream task we would like to build our NLP model is a part of a slightly different distribution and thus  need to fine-tune this Wikitext 103 LM.\n",
    "\n",
    "This step is much faster and it converges much faster as there will be an overlap to the general domain dataset. It only needs to adapt to the idiosyncrasies of the language used and not learn the language per say. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since NLP models are more shallow in comparison to a computer vision model, the fine-tuning approaches need to be different and thus the paper provides novel fine-tuning techniques to do so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminative Fine-tuning\n",
    "\n",
    "Since different layers of the model capture different types of information and thus they should be fine-tuned to different extents. \n",
    "\n",
    "This idea is similar as the use of discriminative learning rates used in CV applications which I explained in detail in my previous [post](https://harish3110.github.io/through-tinted-lenses/image%20classification/2020/04/10/Improving-baseline-model.html#Discriminative-learning-rates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slanted Triangular Learning Rates\n",
    "\n",
    "The idea behind slanted learning rates is that for a pretrained language model to adpat/fine-tune itself to the downstream dataset, the fine-tuning process should ideally converge faster to asuitable region in the parameter space and thern refine its parameters there. \n",
    "\n",
    "So the slanted learning rates approach first linearly increases the learning rates for a short period and then linearly decays the learning rate slowly which is a modification of of Leslie Smith's traingular learning rate approache where the increase and decrease is almost the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1096/1*QptmUluWXteT6oI5bD22rw.png 'Slanted Triangular Learning Rates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a dataloader\n",
    "> Putting the pre-processed data in batches of text sequences for fine-tuning the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                             text\n",
       "0                                                                                             @VirginAmerica What @dhepburn said.\n",
       "1                                                        @VirginAmerica plus you've added commercials to the experience... tacky.\n",
       "2                                                         @VirginAmerica I didn't today... Must mean I need to take another trip!\n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n",
       "4                                                                         @VirginAmerica and it's a really big bad thing about it"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset for fine-tuning language model which only needs the text data\n",
    "\n",
    "df_lm = df[['text']].copy()\n",
    "df_lm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "***Note:*** An important trick used in creating a dataloader here is that we use all the data available to us i.e train and test data. In case we had a dataset with unlabeled reviews we could also use that to fine-tune the pre-trained model better since this step doesn't need labels and is self-supervised. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataloader for self-supervised learning task which tries to predict the next word in a sequence as represented by `text_` below. \n",
    "\n",
    "**Fastai handles text processing steps like tokenization and numericalization internally when `TextBlock` is passed to `DataBlock`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls_lm = DataBlock(\n",
    "    blocks=TextBlock.from_df('text', is_lm=True),\n",
    "    get_x=ColReader('text'), \n",
    "    splitter=RandomSplitter(0.1) \n",
    "    # using only 10% of entire comments data for validation inorder to learn more\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_lm = dls_lm.dataloaders(df_lm, bs=64, seq_len=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "***Note:***\n",
    "\n",
    "- Select the batch size `bs` based on how much your GPU can handle without running out of memory\n",
    "- The sequence length `seq_len` for the data split used here is the default sequence length used for training the Wikipedia 103 language model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos @usairways an already pleasant flight from xxmaj london to xxmaj charlotte ( xxunk ) was made fantastic by an amazing attendant . xxmaj thank you so much xxmaj xxunk ! xxbos @southwestair when is the last chance to get # destinationdragons tickets ? i would die of happiness if i won 😭 🙏 xxbos @americanair my flt is at xxunk tom . i have now xxunk notification that xxmaj i 'm</td>\n",
       "      <td>@usairways an already pleasant flight from xxmaj london to xxmaj charlotte ( xxunk ) was made fantastic by an amazing attendant . xxmaj thank you so much xxmaj xxunk ! xxbos @southwestair when is the last chance to get # destinationdragons tickets ? i would die of happiness if i won 😭 🙏 xxbos @americanair my flt is at xxunk tom . i have now xxunk notification that xxmaj i 'm going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxunk xxbos @united dm'ed you xxbos @virginamerica xxunk , luv xxmaj virgin xxmaj america . xxmaj xxunk xxbos @united how can you never got a flight from austin to xxmaj houston in time for a connection ever ? xxmaj ever ! xxbos @usairways but only for certain flights from xxup clt - nyc . xxmaj the xxunk that you ca n't rebook customers for 48 hours is absurd . xxbos @southwestair xxunk</td>\n",
       "      <td>xxbos @united dm'ed you xxbos @virginamerica xxunk , luv xxmaj virgin xxmaj america . xxmaj xxunk xxbos @united how can you never got a flight from austin to xxmaj houston in time for a connection ever ? xxmaj ever ! xxbos @usairways but only for certain flights from xxup clt - nyc . xxmaj the xxunk that you ca n't rebook customers for 48 hours is absurd . xxbos @southwestair xxunk party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>springs . xxmaj long day starting at a snowy xxunk am in philly . xxbos @americanair i fly xxunk with you and have n't seen one xxbos @usairways xxup ur service is so shitty . xxmaj pilot never showed up so we waited hours because another pilot was supposed to come but did n't # xxmaj cancelled xxmaj flighted xxbos @jetblue xxunk on a xxunk still a xxunk . xxmaj ur new</td>\n",
       "      <td>. xxmaj long day starting at a snowy xxunk am in philly . xxbos @americanair i fly xxunk with you and have n't seen one xxbos @usairways xxup ur service is so shitty . xxmaj pilot never showed up so we waited hours because another pilot was supposed to come but did n't # xxmaj cancelled xxmaj flighted xxbos @jetblue xxunk on a xxunk still a xxunk . xxmaj ur new baggage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls_lm.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataloader for fast use in the future\n",
    "\n",
    "torch.save(dls_lm, path/'airlines_tweets_dls_lm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the Dataloaders in the future\n",
    "\n",
    "dls_lm = torch.load(path/'airlines_tweets_dls_lm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Fine-tuning the language model\n",
    "\n",
    "Fine-tuning Wikitext 103 based LM to airline tweets using ULMFiT fine-tuning methodologies. This fine-tuned LM can thus be used as the base to classify airline texts in the next step.\n",
    "\n",
    "The common metric used in CV models is accuracy but in sequence based models we use something called **perplexity** which is basically exponential of the loss as follows:\n",
    "\n",
    "```\n",
    "torch.exp(cross_entropy)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine-tuning wikitext LM to airline tweets dataset\n",
    "\n",
    "learn = language_model_learner(\n",
    "    dls_lm, AWD_LSTM,\n",
    "    metrics=[accuracy, Perplexity()]).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(4696, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(4696, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1152, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1152, 1152, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1152, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=4696, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Embedding Layer\n",
    "\n",
    "We can see that the above `AWD LSTM` architecture used in ULMFiT has a bunch of layers called **embedding** layers as the input here.\n",
    "\n",
    "The pre-processed text and the batching of data using dataloaders is followed by passing this data into an embedding layer which can be considered as a small neural network by itself which is used to calculate token i.e. word dependencies in the dataset. These layers are trained along with the main neural network model and learns relationships between words in the dataset along the way. \n",
    "\n",
    "An embedding layer is a computationally efficient method to represent tokens in a lesser dimension space, being less sparse and as a look-up table for all tokens in our dataset which captures relationships between the tokens. \n",
    "\n",
    "It's a much more computationally efficient approach to the traditional `one-hot encoding` appraoch which can make these types of task really expensive and inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.fast.ai/images/kittenavalanche.png \"In the above embediing layer learned, vectors for baby animal words are closer together, and an unrelated word like 'avalanche' is further away\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to know more about word embedding check out this amazing [video](https://www.youtube.com/watch?v=25nC0n9ERq4) by Rachael Thomas, co-founder of Fastai.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=0.06309573650360108, lr_steep=0.04786301031708717)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the last layer of the model using a learning rate of `2e-2` based on the above learning rate finder plot using Leslie Smith's [1 Cycle Training](https://arxiv.org/abs/1708.07120) approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.072490</td>\n",
       "      <td>4.019218</td>\n",
       "      <td>0.254360</td>\n",
       "      <td>55.657562</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.152642</td>\n",
       "      <td>3.818886</td>\n",
       "      <td>0.277433</td>\n",
       "      <td>45.553436</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.984446</td>\n",
       "      <td>3.701912</td>\n",
       "      <td>0.291295</td>\n",
       "      <td>40.524731</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.816209</td>\n",
       "      <td>3.644717</td>\n",
       "      <td>0.295096</td>\n",
       "      <td>38.271938</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.690142</td>\n",
       "      <td>3.617772</td>\n",
       "      <td>0.300390</td>\n",
       "      <td>37.254467</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.608074</td>\n",
       "      <td>3.615940</td>\n",
       "      <td>0.301382</td>\n",
       "      <td>37.186298</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fine_tune(5, 2e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have fine-tuned out LM to our downstream task, we save the `encoder` part of the model which portion of the model except the final layer that predicts the next word in the sequence. \n",
    "\n",
    "We can then use this`encoder` part, which is the portion that learns the language semantics, as our base to build an airline tweets classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the encoder\n",
    "\n",
    "learn.save_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Training a classifier on the downstream NLP task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a language model fine-tuned to our downstream NLP dataset we can use the encoder portion of the fine-tuned language model which is the part that learns the features of the language used in the downstream dataset as the base to build a text classifier for tasks such as sentiment analysis, spam detection, fraud detection, document classifcation etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder saved is then appended by a simple classifier consisting of two additional linear blocks consisting of the  standard batch normalization and dropout, with ReLU activations for the intermediate layer and a softmax activation at the last layer for the classification purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning a classifier is a very critical task in a transfer learning method and is the main reason why transfer learning approaches failed until ULMFiT came along. \n",
    "\n",
    "Overly aggressive fine-tuning can result in **catastrophic forgetting** and too cautious fine-tuning can lead to extremely slow convergence. \n",
    "\n",
    "To tackle this problem, ULMFiT introduces a novel fine-tuning technique in **gradual unfreezing** besides also using **slanted triangular learning rates** and **discriminative fine-tuning** to successfully train a classifier using a pre-trained LM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradual Unfreezing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind gradual unfreezing is that fine-tuning a classifier on all layers can result in catastrophic forgetting and thus each layer staring form the las layer is trained one after the other by freezing all the lower layers and only training the layer in question. \n",
    "\n",
    "The paper empirically found that after training the last layer of the model with a learning rate of `lr`, the subsequent layers can be trained one after another by reducing `lr` by a factor of `2.6`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation Throught Time for Text Classification (BPT3C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model architecture for training and fine-tuning the language is that of an LSTM, the paper implements the backpropagation through time(BPTT) approach to be able propagate gradients without them exploding or vanishing. \n",
    "\n",
    "In the ULMFiT approach, a modification to the traditional BPTT is made specifically in the fine-tuning classifier phase called **BPTT for Text Classification(BPT3C)** to make fine-tuning a classifier for large documents feasible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps in BPT3C:\n",
    "- The document is divided into fixed length batches of size 'b'. \n",
    "- At the beginning of each batch, the model is initiated with the final state of the previous batch by keeping track of the hidden states for mean and max-pooling. \n",
    "- The gradients are back-propagated to the batches whose hidden states contributed to the final prediction. \n",
    "- In practice, variable length back-propagation sequences are used. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concat Pooling\n",
    "\n",
    "Since signals for classifying texts can exist anywhere and are not only limited to last word in the sequence, the ULMFiT approach also proposes to concatenate the last time step of the document by max-pooling and mean-pooling representations to provide more signal and better training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the classifier dataloader\n",
    "\n",
    "Ensure that the sequence length and vocab passed to the `TextBlock` is same as that given while fine-tuning LM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                        0.0  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold      name negativereason_gold  retweet_count  \\\n",
       "0                    NaN   cairdin                 NaN              0   \n",
       "1                    NaN  jnardino                 NaN              0   \n",
       "\n",
       "                                                                       text  \\\n",
       "0                                       @VirginAmerica What @dhepburn said.   \n",
       "1  @VirginAmerica plus you've added commercials to the experience... tacky.   \n",
       "\n",
       "  tweet_coord              tweet_created tweet_location  \\\n",
       "0         NaN  2015-02-24 11:35:52 -0800            NaN   \n",
       "1         NaN  2015-02-24 11:15:59 -0800            NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0  Eastern Time (US & Canada)  \n",
       "1  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = (TextBlock.from_df('text', seq_len=dls_lm.seq_len, vocab=dls_lm.vocab), CategoryBlock())\n",
    "dls = DataBlock(blocks=blocks,\n",
    "                get_x=ColReader('text'),\n",
    "                get_y=ColReader('airline_sentiment'),\n",
    "                splitter=RandomSplitter(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = dls.dataloaders(df, bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos @united xxmaj hi have a question re future xxmaj flight xxmaj booking xxmaj problems . xxup dub - jac 29 / 9 xxup jac - lax 8 / 10 xxup lax - dub 13 / 10 . xxmaj i 'm * xxunk xxmaj what is checked bag allowance for xxup jac - lax ?</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos @jetblue i ❤ ️ xxmaj jetblue but i was on flt xxunk from fll to sfo . xxunk off was over 1 hr xxmaj late xxmaj flight , div to phx &amp; &amp; got in 2 hrs xxmaj late xxmaj flight . xxmaj what will be done ?</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos @united xxunk 7 xxup weeks xxmaj late flightr xxup and i xxup still xxup have xxup not xxup received xxup my xxup miles xxup from xxup the mileageplus xxmaj gift xxmaj card $ 150 xxup xxunk xxup card i xxup handed xxup over xxrep 3 !</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(max_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the number of classes for dataloader\n",
    "dls.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11712, 2928)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dls.train_ds), len(dls.valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai2.text.learner.TextLearner at 0x7f29beda0290>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = text_classifier_learner(dls, AWD_LSTM, metrics=[accuracy, FBeta(beta=1, average='weighted')]).to_fp16()\n",
    "learn.load_encoder('finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): SentenceEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(4696, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(4696, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): LinBnDrop(\n",
       "        (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Dropout(p=0.2, inplace=False)\n",
       "        (2): Linear(in_features=1200, out_features=50, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): LinBnDrop(\n",
       "        (0): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Linear(in_features=50, out_features=3, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse\n",
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Training the classifier\n",
    "> Fine-tuning a text classifier using gradual unfreezing, slanted learning rates and discriminating learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.699208</td>\n",
       "      <td>0.511487</td>\n",
       "      <td>0.794399</td>\n",
       "      <td>0.782101</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.621459</td>\n",
       "      <td>0.498418</td>\n",
       "      <td>0.809085</td>\n",
       "      <td>0.805459</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Applying gradual unfreezing of one layer after another\n",
    "\n",
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(1e-3/(2.6**4),1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.545464</td>\n",
       "      <td>0.474213</td>\n",
       "      <td>0.817623</td>\n",
       "      <td>0.806556</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>fbeta_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.470213</td>\n",
       "      <td>0.461382</td>\n",
       "      <td>0.820014</td>\n",
       "      <td>0.810007</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.474566</td>\n",
       "      <td>0.471604</td>\n",
       "      <td>0.825820</td>\n",
       "      <td>0.818866</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),3e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('final_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this post we have seen how to build a fine-tuned language model for any textual data corpus which captures the semantics of the dataset. The encoder part of this fine-tuned language model was then used to build a pretty-decent text classifier that can identify airline tweets with a accuarcy of above 82.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Fastai v2 [Documentation](www.dev.fasta.ai)\n",
    "- Fastbook Chapter 10: [NLP Deep Dive](https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb)\n",
    "- ULMFiT [Paper](https://arxiv.org/abs/1801.06146)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
